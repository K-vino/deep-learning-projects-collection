{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úçÔ∏è GPT-2 for Text Generation\n",
    "\n",
    "Welcome to **GPT-2 text generation**! In this notebook, we'll explore autoregressive language modeling and create an AI that can write stories, poems, code, and more with remarkable creativity and coherence.\n",
    "\n",
    "## What you'll learn:\n",
    "- Autoregressive language modeling\n",
    "- GPT-2 architecture and decoder-only Transformers\n",
    "- Text generation strategies and sampling\n",
    "- Fine-tuning for creative applications\n",
    "\n",
    "Let's generate some amazing text! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install transformers torch datasets gradio\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, GPT2Tokenizer, GPT2Config,\n",
    "    TextDataset, DataCollatorForLanguageModeling,\n",
    "    Trainer, TrainingArguments, pipeline\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "MODEL_NAME = \"gpt2\"  # Can also use \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Loaded {MODEL_NAME}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "# Test tokenization\n",
    "sample_text = \"Once upon a time, in a land far away,\"\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "token_ids = tokenizer.encode(sample_text)\n",
    "\n",
    "print(f\"\\nüî§ Tokenization Example:\")\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Decoded: {tokenizer.decode(token_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text generation pipeline\n",
    "generator = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Test basic generation\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "generated = generator(\n",
    "    prompt,\n",
    "    max_length=100,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.8,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Basic Text Generation:\")\n",
    "print(\"=\" * 50)\n",
    "print(generated[0]['generated_text'])\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced text generation with different sampling strategies\n",
    "def generate_text_advanced(prompt, strategy='top_p', **kwargs):\n",
    "    \"\"\"Generate text with different sampling strategies\"\"\"\n",
    "    \n",
    "    # Default parameters\n",
    "    default_params = {\n",
    "        'max_length': 150,\n",
    "        'num_return_sequences': 1,\n",
    "        'pad_token_id': tokenizer.eos_token_id,\n",
    "        'do_sample': True\n",
    "    }\n",
    "    \n",
    "    # Update with provided parameters\n",
    "    params = {**default_params, **kwargs}\n",
    "    \n",
    "    if strategy == 'greedy':\n",
    "        params['do_sample'] = False\n",
    "    elif strategy == 'top_k':\n",
    "        params['top_k'] = 50\n",
    "        params['temperature'] = 0.8\n",
    "    elif strategy == 'top_p':\n",
    "        params['top_p'] = 0.9\n",
    "        params['temperature'] = 0.8\n",
    "    elif strategy == 'beam_search':\n",
    "        params['num_beams'] = 5\n",
    "        params['do_sample'] = False\n",
    "        params['early_stopping'] = True\n",
    "    \n",
    "    return generator(prompt, **params)\n",
    "\n",
    "# Test different generation strategies\n",
    "prompt = \"In the year 2050, technology will\"\n",
    "strategies = ['greedy', 'top_k', 'top_p', 'beam_search']\n",
    "\n",
    "print(\"üéØ Different Sampling Strategies:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for strategy in strategies:\n",
    "    generated = generate_text_advanced(prompt, strategy=strategy, max_length=120)\n",
    "    print(f\"\\n{strategy.upper()} SAMPLING:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(generated[0]['generated_text'])\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creative text generation examples\n",
    "creative_prompts = {\n",
    "    \"Story\": \"Once upon a time, in a magical forest where trees could talk,\",\n",
    "    \"Poem\": \"Roses are red, violets are blue,\",\n",
    "    \"Science Fiction\": \"The spaceship landed on the mysterious planet, and the crew discovered\",\n",
    "    \"Recipe\": \"To make the perfect chocolate cake, you will need\",\n",
    "    \"News Article\": \"Breaking news: Scientists have just announced a groundbreaking discovery\"\n",
    "}\n",
    "\n",
    "print(\"üé® Creative Text Generation Examples:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for category, prompt in creative_prompts.items():\n",
    "    generated = generate_text_advanced(\n",
    "        prompt, \n",
    "        strategy='top_p',\n",
    "        max_length=200,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìù {category.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(generated[0]['generated_text'])\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature analysis - creativity vs coherence\n",
    "def analyze_temperature_effects(prompt, temperatures=[0.1, 0.5, 1.0, 1.5, 2.0]):\n",
    "    \"\"\"Analyze how temperature affects generation quality\"\"\"\n",
    "    \n",
    "    print(f\"üå°Ô∏è Temperature Analysis for prompt: '{prompt}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        generated = generator(\n",
    "            prompt,\n",
    "            max_length=100,\n",
    "            temperature=temp,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTemperature {temp}:\")\n",
    "        print(\"-\" * 20)\n",
    "        print(generated[0]['generated_text'])\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "# Analyze temperature effects\n",
    "analyze_temperature_effects(\"The secret to happiness is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom training data for fine-tuning\n",
    "def create_custom_dataset():\n",
    "    \"\"\"Create a custom dataset for fine-tuning\"\"\"\n",
    "    \n",
    "    # Sample creative writing dataset\n",
    "    stories = [\n",
    "        \"The old lighthouse keeper had seen many storms, but none like this one. The waves crashed against the rocks with unprecedented fury, and the wind howled like a banshee. Yet, he remained at his post, knowing that ships depended on his light to guide them safely to shore.\",\n",
    "        \n",
    "        \"In the heart of the ancient library, between dusty tomes and forgotten scrolls, lived a small dragon named Ember. Unlike his fierce relatives, Ember preferred reading to roaring, and his favorite pastime was organizing books by their magical properties.\",\n",
    "        \n",
    "        \"The time traveler checked her pocket watch one last time before stepping into the swirling portal. She had one chance to prevent the catastrophe that would reshape history, but changing the past always came with unexpected consequences.\",\n",
    "        \n",
    "        \"Professor Chen's latest invention hummed quietly in the corner of her laboratory. The device could translate any language in the universe, but she had just discovered it was also picking up signals from civilizations that shouldn't exist.\",\n",
    "        \n",
    "        \"The garden grew in impossible ways - flowers bloomed in winter, trees bore fruit out of season, and the paths rearranged themselves when no one was looking. The gardener smiled, knowing that magic required patience and understanding.\"\n",
    "    ] * 20  # Repeat for more training data\n",
    "    \n",
    "    return stories\n",
    "\n",
    "# Create dataset\n",
    "custom_stories = create_custom_dataset()\n",
    "print(f\"Created custom dataset with {len(custom_stories)} stories\")\n",
    "print(f\"\\nSample story:\")\n",
    "print(custom_stories[0][:200] + \"...\")\n",
    "\n",
    "# Save to file for training\n",
    "with open('custom_stories.txt', 'w', encoding='utf-8') as f:\n",
    "    for story in custom_stories:\n",
    "        f.write(story + '\\n\\n')\n",
    "\n",
    "print(\"\\nüíæ Custom dataset saved to 'custom_stories.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive text generation function\n",
    "def interactive_generation():\n",
    "    \"\"\"Interactive text generation with user prompts\"\"\"\n",
    "    \n",
    "    print(\"üéÆ Interactive Text Generation\")\n",
    "    print(\"Enter prompts and see GPT-2 complete them!\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            prompt = input(\"Enter your prompt: \")\n",
    "            \n",
    "            if prompt.lower() == 'quit':\n",
    "                break\n",
    "            \n",
    "            if not prompt.strip():\n",
    "                continue\n",
    "            \n",
    "            # Get generation parameters\n",
    "            print(\"\\nGeneration options:\")\n",
    "            print(\"1. Creative (high temperature)\")\n",
    "            print(\"2. Balanced (medium temperature)\")\n",
    "            print(\"3. Conservative (low temperature)\")\n",
    "            \n",
    "            choice = input(\"Choose option (1-3, default=2): \").strip() or '2'\n",
    "            \n",
    "            # Set parameters based on choice\n",
    "            if choice == '1':\n",
    "                temp, top_p = 1.2, 0.95\n",
    "            elif choice == '3':\n",
    "                temp, top_p = 0.5, 0.8\n",
    "            else:\n",
    "                temp, top_p = 0.8, 0.9\n",
    "            \n",
    "            # Generate text\n",
    "            generated = generator(\n",
    "                prompt,\n",
    "                max_length=200,\n",
    "                temperature=temp,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"GENERATED TEXT:\")\n",
    "            print(\"=\"*60)\n",
    "            print(generated[0]['generated_text'])\n",
    "            print(\"=\"*60 + \"\\n\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"Thanks for using the interactive generator!\")\n",
    "\n",
    "# Note: Uncomment the line below to run interactive mode\n",
    "# interactive_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance and generation quality\n",
    "def analyze_generation_quality(prompts, num_samples=3):\n",
    "    \"\"\"Analyze generation quality across different prompts\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nüìä Analyzing prompt: '{prompt[:50]}...'\")\n",
    "        \n",
    "        generations = []\n",
    "        for i in range(num_samples):\n",
    "            generated = generator(\n",
    "                prompt,\n",
    "                max_length=150,\n",
    "                temperature=0.8,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            generations.append(generated[0]['generated_text'])\n",
    "        \n",
    "        # Calculate diversity (unique words)\n",
    "        all_words = []\n",
    "        for gen in generations:\n",
    "            words = gen.lower().split()\n",
    "            all_words.extend(words)\n",
    "        \n",
    "        unique_words = len(set(all_words))\n",
    "        total_words = len(all_words)\n",
    "        diversity = unique_words / total_words if total_words > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'prompt': prompt,\n",
    "            'diversity': diversity,\n",
    "            'avg_length': np.mean([len(gen.split()) for gen in generations]),\n",
    "            'generations': generations\n",
    "        })\n",
    "        \n",
    "        print(f\"Diversity score: {diversity:.3f}\")\n",
    "        print(f\"Average length: {results[-1]['avg_length']:.1f} words\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test prompts for analysis\n",
    "test_prompts = [\n",
    "    \"The future of space exploration\",\n",
    "    \"A day in the life of a robot\",\n",
    "    \"The most important lesson I learned\"\n",
    "]\n",
    "\n",
    "quality_results = analyze_generation_quality(test_prompts)\n",
    "\n",
    "# Visualize results\n",
    "if quality_results:\n",
    "    diversities = [r['diversity'] for r in quality_results]\n",
    "    lengths = [r['avg_length'] for r in quality_results]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Diversity scores\n",
    "    ax1.bar(range(len(diversities)), diversities)\n",
    "    ax1.set_title('üìä Generation Diversity Scores')\n",
    "    ax1.set_xlabel('Prompt Index')\n",
    "    ax1.set_ylabel('Diversity Score')\n",
    "    ax1.set_xticks(range(len(diversities)))\n",
    "    \n",
    "    # Average lengths\n",
    "    ax2.bar(range(len(lengths)), lengths, color='orange')\n",
    "    ax2.set_title('üìè Average Generation Length')\n",
    "    ax2.set_xlabel('Prompt Index')\n",
    "    ax2.set_ylabel('Average Words')\n",
    "    ax2.set_xticks(range(len(lengths)))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nüìä GPT-2 Analysis Summary:\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")\n",
    "print(f\"Vocabulary: {tokenizer.vocab_size:,} tokens\")\n",
    "print(f\"Average diversity: {np.mean(diversities):.3f}\")\n",
    "print(f\"Average length: {np.mean(lengths):.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully explored GPT-2 for text generation! Here's what you've accomplished:\n",
    "\n",
    "‚úÖ **GPT-2 Usage**: Loaded and used pre-trained language models  \n",
    "‚úÖ **Generation Strategies**: Explored different sampling methods  \n",
    "‚úÖ **Creative Applications**: Generated stories, poems, and more  \n",
    "‚úÖ **Parameter Tuning**: Controlled creativity with temperature  \n",
    "‚úÖ **Quality Analysis**: Measured generation diversity and coherence  \n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. Fine-tune GPT-2 on your custom dataset\n",
    "2. Try larger models (GPT-2 Medium/Large)\n",
    "3. Implement controllable generation\n",
    "4. Move on to **Project 12: Diffusion Models for Image Generation**\n",
    "\n",
    "Ready for the final project? Let's generate images with diffusion! üé®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
