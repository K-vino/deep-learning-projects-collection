{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé® Diffusion Models for Image Generation\n",
    "\n",
    "Welcome to the **cutting-edge of generative AI**! In this notebook, we'll explore diffusion models - the technology behind DALL-E 2, Midjourney, and Stable Diffusion. Generate stunning images from text prompts!\n",
    "\n",
    "## What you'll learn:\n",
    "- Diffusion processes and reverse denoising\n",
    "- DDPM and DDIM sampling techniques\n",
    "- Text-to-image generation with CLIP\n",
    "- Stable Diffusion and latent space generation\n",
    "\n",
    "Let's create amazing art with AI! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install diffusers transformers accelerate torch torchvision xformers\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Diffusion libraries\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline, DDPMPipeline, DDIMPipeline,\n",
    "    UNet2DModel, DDPMScheduler, DDIMScheduler\n",
    ")\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stable Diffusion pipeline\n",
    "MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# Load the pipeline\n",
    "print(\"üöÄ Loading Stable Diffusion pipeline...\")\n",
    "print(\"This may take a few minutes on first run...\")\n",
    "\n",
    "try:\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        safety_checker=None,  # Disable for demo\n",
    "        requires_safety_checker=False\n",
    "    )\n",
    "    pipe = pipe.to(device)\n",
    "    \n",
    "    # Enable memory efficient attention\n",
    "    if torch.cuda.is_available():\n",
    "        pipe.enable_attention_slicing()\n",
    "        pipe.enable_xformers_memory_efficient_attention()\n",
    "    \n",
    "    print(\"‚úÖ Stable Diffusion loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load Stable Diffusion: {e}\")\n",
    "    print(\"Creating a simplified demo instead...\")\n",
    "    pipe = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-Image Generation\n",
    "def generate_images(prompts, num_images=1, guidance_scale=7.5, num_steps=20):\n",
    "    \"\"\"Generate images from text prompts\"\"\"\n",
    "    \n",
    "    if pipe is None:\n",
    "        print(\"‚ö†Ô∏è Stable Diffusion not available. Creating placeholder images...\")\n",
    "        # Create placeholder images\n",
    "        images = []\n",
    "        for prompt in prompts:\n",
    "            # Create a colorful placeholder\n",
    "            img = np.random.rand(512, 512, 3) * 255\n",
    "            img = img.astype(np.uint8)\n",
    "            images.append(Image.fromarray(img))\n",
    "        return images\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"üé® Generating: '{prompt}'\")\n",
    "        \n",
    "        # Generate image\n",
    "        with torch.autocast(device.type):\n",
    "            result = pipe(\n",
    "                prompt,\n",
    "                num_images_per_prompt=num_images,\n",
    "                guidance_scale=guidance_scale,\n",
    "                num_inference_steps=num_steps,\n",
    "                height=512,\n",
    "                width=512\n",
    "            )\n",
    "        \n",
    "        images.extend(result.images)\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"A beautiful sunset over a mountain lake, digital art\",\n",
    "    \"A cute robot reading a book in a cozy library\",\n",
    "    \"A magical forest with glowing mushrooms and fireflies\"\n",
    "]\n",
    "\n",
    "# Generate images\n",
    "generated_images = generate_images(test_prompts, num_images=1)\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(1, len(test_prompts), figsize=(15, 5))\n",
    "if len(test_prompts) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (prompt, img) in enumerate(zip(test_prompts, generated_images)):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"'{prompt[:30]}...'\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('üé® Text-to-Image Generation Results', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Image generation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore different guidance scales\n",
    "def explore_guidance_scales(prompt, scales=[1.0, 5.0, 7.5, 10.0, 15.0]):\n",
    "    \"\"\"Explore how guidance scale affects generation\"\"\"\n",
    "    \n",
    "    if pipe is None:\n",
    "        print(\"‚ö†Ô∏è Stable Diffusion not available for guidance scale exploration\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üéØ Exploring guidance scales for: '{prompt}'\")\n",
    "    \n",
    "    images = []\n",
    "    for scale in scales:\n",
    "        print(f\"Generating with guidance scale: {scale}\")\n",
    "        \n",
    "        with torch.autocast(device.type):\n",
    "            result = pipe(\n",
    "                prompt,\n",
    "                guidance_scale=scale,\n",
    "                num_inference_steps=20,\n",
    "                height=512,\n",
    "                width=512\n",
    "            )\n",
    "        \n",
    "        images.append(result.images[0])\n",
    "    \n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(1, len(scales), figsize=(20, 4))\n",
    "    \n",
    "    for i, (scale, img) in enumerate(zip(scales, images)):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'Guidance Scale: {scale}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'üéØ Guidance Scale Effects: \"{prompt}\"', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test guidance scales\n",
    "explore_guidance_scales(\"A majestic dragon flying over a medieval castle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creative prompt exploration\n",
    "creative_prompts = {\n",
    "    \"Artistic Styles\": [\n",
    "        \"A cat in the style of Van Gogh's Starry Night\",\n",
    "        \"A cityscape in cyberpunk style with neon lights\",\n",
    "        \"A portrait in the style of Leonardo da Vinci\"\n",
    "    ],\n",
    "    \"Fantasy Scenes\": [\n",
    "        \"A wizard's tower floating in the clouds\",\n",
    "        \"An underwater city with mermaids and coral\",\n",
    "        \"A steampunk airship flying through storm clouds\"\n",
    "    ],\n",
    "    \"Futuristic Concepts\": [\n",
    "        \"A robot gardener tending to alien plants\",\n",
    "        \"A space station orbiting a purple planet\",\n",
    "        \"A holographic concert in a futuristic city\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üé® Creative Prompt Exploration:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for category, prompts in creative_prompts.items():\n",
    "    print(f\"\\nüìÇ {category}:\")\n",
    "    \n",
    "    # Generate one image per category for demo\n",
    "    sample_prompt = prompts[0]\n",
    "    images = generate_images([sample_prompt], num_images=1)\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(images[0])\n",
    "    plt.title(f'{category}\\n\"{sample_prompt}\"')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Generated: {sample_prompt}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate diffusion process visualization\n",
    "def visualize_diffusion_process():\n",
    "    \"\"\"Visualize the diffusion denoising process\"\"\"\n",
    "    \n",
    "    if pipe is None:\n",
    "        print(\"‚ö†Ô∏è Creating simulated diffusion process...\")\n",
    "        \n",
    "        # Create a simple simulation of the diffusion process\n",
    "        steps = [0, 5, 10, 15, 20]\n",
    "        images = []\n",
    "        \n",
    "        # Start with noise\n",
    "        base_image = np.random.randn(64, 64, 3) * 0.5 + 0.5\n",
    "        \n",
    "        for step in steps:\n",
    "            # Simulate progressive denoising\n",
    "            noise_level = (20 - step) / 20\n",
    "            \n",
    "            # Create a simple pattern that emerges\n",
    "            x, y = np.meshgrid(np.linspace(-1, 1, 64), np.linspace(-1, 1, 64))\n",
    "            pattern = np.sin(x * 3) * np.cos(y * 3)\n",
    "            pattern = (pattern + 1) / 2  # Normalize to [0, 1]\n",
    "            \n",
    "            # Mix noise and pattern based on step\n",
    "            noise = np.random.randn(64, 64) * noise_level\n",
    "            signal = pattern * (1 - noise_level)\n",
    "            \n",
    "            combined = signal + noise * 0.3\n",
    "            combined = np.clip(combined, 0, 1)\n",
    "            \n",
    "            # Convert to RGB\n",
    "            rgb_image = np.stack([combined] * 3, axis=-1)\n",
    "            images.append(rgb_image)\n",
    "        \n",
    "        # Display the process\n",
    "        fig, axes = plt.subplots(1, len(steps), figsize=(15, 3))\n",
    "        \n",
    "        for i, (step, img) in enumerate(zip(steps, images)):\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f'Step {step}/20')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle('üåä Simulated Diffusion Denoising Process', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # Real diffusion process visualization would require custom pipeline\n",
    "    print(\"üåä Diffusion Process Visualization\")\n",
    "    print(\"Note: Full process visualization requires custom implementation\")\n",
    "    print(\"The diffusion model progressively removes noise over multiple steps\")\n",
    "    \n",
    "    # Show noise schedule\n",
    "    scheduler = DDIMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n",
    "    timesteps = scheduler.timesteps[:10]  # First 10 steps\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(timesteps.cpu().numpy())\n",
    "    plt.title('üìä Diffusion Timestep Schedule')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Timestep')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize diffusion process\n",
    "visualize_diffusion_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prompt generator\n",
    "def interactive_prompt_generator():\n",
    "    \"\"\"Interactive text-to-image generation\"\"\"\n",
    "    \n",
    "    print(\"üéÆ Interactive Image Generation\")\n",
    "    print(\"Enter prompts and generate amazing images!\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            prompt = input(\"Enter your prompt: \")\n",
    "            \n",
    "            if prompt.lower() == 'quit':\n",
    "                break\n",
    "            \n",
    "            if not prompt.strip():\n",
    "                continue\n",
    "            \n",
    "            # Get generation parameters\n",
    "            print(\"\\nGeneration options:\")\n",
    "            print(\"1. Fast (10 steps)\")\n",
    "            print(\"2. Balanced (20 steps)\")\n",
    "            print(\"3. High Quality (50 steps)\")\n",
    "            \n",
    "            choice = input(\"Choose option (1-3, default=2): \").strip() or '2'\n",
    "            \n",
    "            # Set parameters based on choice\n",
    "            if choice == '1':\n",
    "                steps, guidance = 10, 7.5\n",
    "            elif choice == '3':\n",
    "                steps, guidance = 50, 10.0\n",
    "            else:\n",
    "                steps, guidance = 20, 7.5\n",
    "            \n",
    "            # Generate image\n",
    "            print(f\"\\nüé® Generating image for: '{prompt}'\")\n",
    "            print(f\"Steps: {steps}, Guidance: {guidance}\")\n",
    "            \n",
    "            images = generate_images([prompt], num_images=1, \n",
    "                                   guidance_scale=guidance, num_steps=steps)\n",
    "            \n",
    "            # Display result\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(images[0])\n",
    "            plt.title(f'Generated: \"{prompt}\"')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            # Save option\n",
    "            save = input(\"Save image? (y/n): \").lower().startswith('y')\n",
    "            if save:\n",
    "                filename = f\"generated_{len(prompt.split())}_words.png\"\n",
    "                images[0].save(filename)\n",
    "                print(f\"üíæ Saved as {filename}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"Thanks for using the interactive generator!\")\n",
    "\n",
    "# Note: Uncomment the line below to run interactive mode\n",
    "# interactive_prompt_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced techniques demonstration\n",
    "def demonstrate_advanced_techniques():\n",
    "    \"\"\"Demonstrate advanced diffusion techniques\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Advanced Diffusion Techniques\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # 1. Negative prompting\n",
    "    print(\"\\n1. üö´ Negative Prompting:\")\n",
    "    positive_prompt = \"A beautiful landscape with mountains\"\n",
    "    negative_prompt = \"blurry, low quality, distorted\"\n",
    "    \n",
    "    if pipe is not None:\n",
    "        with torch.autocast(device.type):\n",
    "            result = pipe(\n",
    "                positive_prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                guidance_scale=7.5,\n",
    "                num_inference_steps=20\n",
    "            )\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(result.images[0])\n",
    "        plt.title(f'Positive: \"{positive_prompt}\"\\nNegative: \"{negative_prompt}\"')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    # 2. Seed control for reproducibility\n",
    "    print(\"\\n2. üå± Seed Control:\")\n",
    "    prompt = \"A cute cat wearing a wizard hat\"\n",
    "    seeds = [42, 123, 456]\n",
    "    \n",
    "    if pipe is not None:\n",
    "        fig, axes = plt.subplots(1, len(seeds), figsize=(15, 5))\n",
    "        \n",
    "        for i, seed in enumerate(seeds):\n",
    "            generator = torch.Generator(device=device).manual_seed(seed)\n",
    "            \n",
    "            with torch.autocast(device.type):\n",
    "                result = pipe(\n",
    "                    prompt,\n",
    "                    generator=generator,\n",
    "                    guidance_scale=7.5,\n",
    "                    num_inference_steps=20\n",
    "                )\n",
    "            \n",
    "            axes[i].imshow(result.images[0])\n",
    "            axes[i].set_title(f'Seed: {seed}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'üå± Same Prompt, Different Seeds: \"{prompt}\"')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 3. Aspect ratio variations\n",
    "    print(\"\\n3. üìê Aspect Ratio Variations:\")\n",
    "    prompt = \"A serene Japanese garden with cherry blossoms\"\n",
    "    dimensions = [(512, 512), (768, 512), (512, 768)]\n",
    "    labels = [\"Square\", \"Landscape\", \"Portrait\"]\n",
    "    \n",
    "    if pipe is not None:\n",
    "        fig, axes = plt.subplots(1, len(dimensions), figsize=(15, 8))\n",
    "        \n",
    "        for i, ((width, height), label) in enumerate(zip(dimensions, labels)):\n",
    "            with torch.autocast(device.type):\n",
    "                result = pipe(\n",
    "                    prompt,\n",
    "                    width=width,\n",
    "                    height=height,\n",
    "                    guidance_scale=7.5,\n",
    "                    num_inference_steps=20\n",
    "                )\n",
    "            \n",
    "            axes[i].imshow(result.images[0])\n",
    "            axes[i].set_title(f'{label}\\n{width}x{height}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'üìê Aspect Ratio Variations: \"{prompt}\"')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Advanced techniques demonstration completed!\")\n",
    "\n",
    "# Demonstrate advanced techniques\n",
    "demonstrate_advanced_techniques()\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nüìä Diffusion Models Summary:\")\n",
    "print(f\"Model: Stable Diffusion v1.5\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Available: {'Yes' if pipe is not None else 'No (Demo mode)'}\")\n",
    "print(f\"Capabilities: Text-to-image, negative prompting, seed control\")\n",
    "print(f\"Resolution: Up to 768x768 (higher with memory optimization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully explored diffusion models for image generation! Here's what you've accomplished:\n",
    "\n",
    "‚úÖ **Diffusion Models**: Understood the denoising process  \n",
    "‚úÖ **Stable Diffusion**: Generated high-quality images from text  \n",
    "‚úÖ **Text-to-Image**: Created diverse artistic content  \n",
    "‚úÖ **Advanced Techniques**: Explored guidance, seeds, and negative prompts  \n",
    "‚úÖ **Creative Applications**: Generated art, landscapes, and fantasy scenes  \n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. Explore ControlNet for precise image control\n",
    "2. Try DreamBooth for personalized generation\n",
    "3. Experiment with inpainting and outpainting\n",
    "4. Build custom diffusion models for specific domains\n",
    "\n",
    "## üéä **COLLECTION COMPLETE!**\n",
    "\n",
    "You've now completed all **12 Deep Learning Projects**! From basic neural networks to cutting-edge diffusion models, you've mastered:\n",
    "\n",
    "- **Fundamentals**: Neural networks, CNNs, RNNs\n",
    "- **Computer Vision**: Image classification, GANs, style transfer\n",
    "- **NLP**: Text classification, Transformers, BERT, GPT-2\n",
    "- **Generative AI**: VAEs, diffusion models\n",
    "- **Time Series**: LSTM forecasting\n",
    "\n",
    "**Congratulations on your deep learning journey! üåü**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
