{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Transformer for Language Modeling\n",
    "\n",
    "Welcome to the **Transformer revolution**! In this notebook, we'll build the complete Transformer architecture from scratch and create a powerful language model using self-attention mechanisms.\n",
    "\n",
    "## What you'll learn:\n",
    "- Multi-head self-attention mechanism\n",
    "- Positional encoding and layer normalization\n",
    "- Complete Transformer architecture\n",
    "- Language modeling and text generation\n",
    "\n",
    "Let's revolutionize NLP! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample text dataset\n",
    "def create_sample_text():\n",
    "    \"\"\"Create a sample text dataset for language modeling\"\"\"\n",
    "    sample_texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"To be or not to be, that is the question.\",\n",
    "        \"In the beginning was the Word, and the Word was with God.\",\n",
    "        \"It was the best of times, it was the worst of times.\",\n",
    "        \"All happy families are alike; each unhappy family is unhappy in its own way.\",\n",
    "        \"Call me Ishmael. Some years agoâ€”never mind how long precisely.\",\n",
    "        \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\",\n",
    "        \"In a hole in the ground there lived a hobbit.\",\n",
    "        \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
    "        \"Space: the final frontier. These are the voyages of the starship Enterprise.\"\n",
    "    ] * 100  # Repeat for more training data\n",
    "    \n",
    "    return \" \".join(sample_texts)\n",
    "\n",
    "# Load and preprocess text\n",
    "text = create_sample_text()\n",
    "print(f\"Text length: {len(text)} characters\")\n",
    "print(f\"Sample: {text[:200]}...\")\n",
    "\n",
    "# Create character-level vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars)}\")\n",
    "\n",
    "# Convert text to sequences\n",
    "def text_to_sequences(text, seq_length=64):\n",
    "    \"\"\"Convert text to input-target sequences\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(0, len(text) - seq_length, seq_length // 2):\n",
    "        seq = text[i:i + seq_length]\n",
    "        target = text[i + 1:i + seq_length + 1]\n",
    "        \n",
    "        if len(seq) == seq_length and len(target) == seq_length:\n",
    "            sequences.append([char_to_idx[ch] for ch in seq])\n",
    "            targets.append([char_to_idx[ch] for ch in target])\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "SEQ_LENGTH = 64\n",
    "X, y = text_to_sequences(text, SEQ_LENGTH)\n",
    "\n",
    "print(f\"\\nSequences shape: {X.shape}\")\n",
    "print(f\"Targets shape: {y.shape}\")\n",
    "print(f\"Number of training sequences: {len(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"Generate positional encoding matrix\"\"\"\n",
    "    pos_enc = np.zeros((seq_len, d_model))\n",
    "    \n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            pos_enc[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))\n",
    "    \n",
    "    return pos_enc\n",
    "\n",
    "# Visualize positional encoding\n",
    "d_model = 128\n",
    "pos_enc = get_positional_encoding(SEQ_LENGTH, d_model)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(pos_enc.T, cmap='RdYlBu', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title('ðŸŒŠ Positional Encoding Pattern')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Encoding Dimension')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Positional encoding shape: {pos_enc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention Layer\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth)\"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scaled_attention, attention_weights = self.scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                    (batch_size, -1, self.d_model))\n",
    "        \n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
    "        \"\"\"Calculate the attention weights\"\"\"\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        \n",
    "        # Scale matmul_qk\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        \n",
    "        # Add the mask to the scaled tensor\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "        # Softmax is normalized on the last axis (seq_len_k)\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        \n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "print(\"âœ… Multi-Head Attention layer defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask=None):\n",
    "        attn_output, attention_weights = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2, attention_weights\n",
    "\n",
    "print(\"âœ… Transformer Block defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Transformer Model\n",
    "class TransformerLanguageModel(keras.Model):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, dff, num_layers, \n",
    "                 maximum_position_encoding, rate=0.1, **kwargs):\n",
    "        super(TransformerLanguageModel, self).__init__(**kwargs)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = get_positional_encoding(maximum_position_encoding, d_model)\n",
    "        \n",
    "        self.transformer_blocks = [TransformerBlock(d_model, num_heads, dff, rate) \n",
    "                                 for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = layers.Dropout(rate)\n",
    "        self.final_layer = layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, x, training, mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for i, transformer_block in enumerate(self.transformer_blocks):\n",
    "            x, attn_weights = transformer_block(x, training, mask)\n",
    "            attention_weights[f'transformer_block_{i+1}'] = attn_weights\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = self.final_layer(x)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Create model\n",
    "D_MODEL = 128\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "NUM_LAYERS = 4\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "model = TransformerLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dff=DFF,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    maximum_position_encoding=SEQ_LENGTH,\n",
    "    rate=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "print(f\"âœ… Transformer Language Model created!\")\n",
    "print(f\"Model parameters: D_MODEL={D_MODEL}, NUM_HEADS={NUM_HEADS}, NUM_LAYERS={NUM_LAYERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create causal mask for autoregressive training\n",
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"Create mask to prevent attention to future tokens\"\"\"\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "# Loss and metrics\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "# Optimizer with learning rate scheduling\n",
    "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "optimizer = keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "print(\"âœ… Loss function and optimizer defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar_inp)[1])\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = model(tar_inp, training=True, mask=look_ahead_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Track losses\n",
    "train_losses = []\n",
    "\n",
    "print(f\"ðŸš€ Starting Transformer training...\")\n",
    "print(f\"Epochs: {EPOCHS}, Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = []\n",
    "    \n",
    "    for batch, (inp, tar) in enumerate(dataset):\n",
    "        loss = train_step(inp, tar)\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {loss:.4f}')\n",
    "    \n",
    "    avg_loss = tf.reduce_mean(epoch_loss)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}: Average Loss = {avg_loss:.4f}')\n",
    "\n",
    "print(\"\\nðŸŽ‰ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation function\n",
    "def generate_text(model, start_string, num_generate=100, temperature=1.0):\n",
    "    \"\"\"Generate text using the trained model\"\"\"\n",
    "    input_eval = [char_to_idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions, _ = model(input_eval, training=False)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # Use temperature to control randomness\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        # Add predicted character to input for next iteration\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx_to_char[predicted_id])\n",
    "    \n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "# Generate sample text\n",
    "print(\"ðŸŽ­ Generated Text Samples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    generated = generate_text(model, \"The \", num_generate=200, temperature=temp)\n",
    "    print(f\"\\nTemperature {temp}:\")\n",
    "    print(generated)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Visualize training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses)\n",
    "plt.title('ðŸ“‰ Transformer Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Training Summary:\")\n",
    "print(f\"Final Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Model Parameters: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"Sequence Length: {SEQ_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've successfully built a complete Transformer from scratch! Here's what you've accomplished:\n",
    "\n",
    "âœ… **Multi-Head Attention**: Implemented the core attention mechanism  \n",
    "âœ… **Positional Encoding**: Added position information to sequences  \n",
    "âœ… **Transformer Blocks**: Built complete encoder layers  \n",
    "âœ… **Language Modeling**: Created an autoregressive text generator  \n",
    "âœ… **Training**: Optimized with custom learning rate scheduling  \n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "1. Try larger models with more layers and heads\n",
    "2. Implement BERT-style bidirectional encoding\n",
    "3. Experiment with different attention patterns\n",
    "4. Move on to **Project 10: Fine-tuning BERT for Sentiment Analysis**\n",
    "\n",
    "Ready for transfer learning with BERT? Let's fine-tune! ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
