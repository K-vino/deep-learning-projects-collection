{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé® Neural Style Transfer\n",
    "\n",
    "Welcome to **Neural Style Transfer**! In this notebook, we'll combine the content of one image with the artistic style of another using deep CNNs. Transform your photos into masterpieces!\n",
    "\n",
    "## What you'll learn:\n",
    "- How CNNs extract content and style features\n",
    "- Gram matrices for style representation\n",
    "- Optimization-based image generation\n",
    "- Fast style transfer techniques\n",
    "\n",
    "Let's create digital art! üñºÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('images', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample images for demonstration\n",
    "def create_sample_content_image(size=(512, 512)):\n",
    "    \"\"\"Create a sample content image\"\"\"\n",
    "    # Create a landscape-like image\n",
    "    img = np.zeros((*size, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Sky gradient (blue to light blue)\n",
    "    for i in range(size[0] // 2):\n",
    "        intensity = int(100 + (i / (size[0] // 2)) * 155)\n",
    "        img[i, :] = [135, 206, intensity]  # Sky blue gradient\n",
    "    \n",
    "    # Ground (green)\n",
    "    for i in range(size[0] // 2, size[0]):\n",
    "        img[i, :] = [34, 139, 34]  # Forest green\n",
    "    \n",
    "    # Add some geometric shapes (buildings/trees)\n",
    "    # Building 1\n",
    "    img[300:450, 100:200] = [169, 169, 169]  # Gray building\n",
    "    # Building 2\n",
    "    img[250:450, 300:380] = [105, 105, 105]  # Darker building\n",
    "    # Tree\n",
    "    img[350:450, 450:500] = [139, 69, 19]   # Tree trunk\n",
    "    img[300:380, 430:520] = [0, 100, 0]     # Tree leaves\n",
    "    \n",
    "    return img\n",
    "\n",
    "def create_sample_style_image(size=(512, 512)):\n",
    "    \"\"\"Create a sample style image with artistic patterns\"\"\"\n",
    "    img = np.zeros((*size, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Create swirling pattern (Van Gogh-like)\n",
    "    center_x, center_y = size[0] // 2, size[1] // 2\n",
    "    \n",
    "    for i in range(size[0]):\n",
    "        for j in range(size[1]):\n",
    "            # Distance from center\n",
    "            dx, dy = i - center_x, j - center_y\n",
    "            distance = np.sqrt(dx**2 + dy**2)\n",
    "            angle = np.arctan2(dy, dx)\n",
    "            \n",
    "            # Create swirl pattern\n",
    "            swirl = np.sin(distance * 0.02 + angle * 3) * 0.5 + 0.5\n",
    "            \n",
    "            # Color based on swirl pattern\n",
    "            if swirl > 0.7:\n",
    "                img[i, j] = [255, 215, 0]    # Gold\n",
    "            elif swirl > 0.4:\n",
    "                img[i, j] = [30, 144, 255]   # Dodger blue\n",
    "            else:\n",
    "                img[i, j] = [138, 43, 226]   # Blue violet\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Create sample images\n",
    "content_img = create_sample_content_image()\n",
    "style_img = create_sample_style_image()\n",
    "\n",
    "# Save images\n",
    "Image.fromarray(content_img).save('images/content.jpg')\n",
    "Image.fromarray(style_img).save('images/style.jpg')\n",
    "\n",
    "# Display images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(content_img)\n",
    "axes[0].set_title('üì∑ Content Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(style_img)\n",
    "axes[1].set_title('üé® Style Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Sample images created and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing functions\n",
    "def load_and_process_img(path_to_img, max_dim=512):\n",
    "    \"\"\"Load and preprocess image for style transfer\"\"\"\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    \n",
    "    # Resize image\n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    long_dim = max(shape)\n",
    "    scale = max_dim / long_dim\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "    \n",
    "    img = tf.image.resize(img, new_shape)\n",
    "    img = img[tf.newaxis, :]\n",
    "    return img\n",
    "\n",
    "def deprocess_img(processed_img):\n",
    "    \"\"\"Deprocess image for display\"\"\"\n",
    "    x = processed_img.copy()\n",
    "    if len(x.shape) == 4:\n",
    "        x = np.squeeze(x, 0)\n",
    "    \n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    \n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "# Load and preprocess images\n",
    "content_image = load_and_process_img('images/content.jpg')\n",
    "style_image = load_and_process_img('images/style.jpg')\n",
    "\n",
    "print(f\"Content image shape: {content_image.shape}\")\n",
    "print(f\"Style image shape: {style_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build style transfer model using VGG19\n",
    "def get_model():\n",
    "    \"\"\"Create a VGG19 model with access to intermediate layers\"\"\"\n",
    "    # Load VGG19 without top layers\n",
    "    vgg = VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "    \n",
    "    # Content and style layers\n",
    "    content_layers = ['block5_conv2']\n",
    "    style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', \n",
    "                   'block4_conv1', 'block5_conv1']\n",
    "    \n",
    "    # Get outputs from specified layers\n",
    "    outputs = [vgg.get_layer(name).output for name in style_layers + content_layers]\n",
    "    model = models.Model([vgg.input], outputs)\n",
    "    \n",
    "    return model, style_layers, content_layers\n",
    "\n",
    "# Create model\n",
    "model, style_layers, content_layers = get_model()\n",
    "num_style_layers = len(style_layers)\n",
    "num_content_layers = len(content_layers)\n",
    "\n",
    "print(f\"Style layers: {style_layers}\")\n",
    "print(f\"Content layers: {content_layers}\")\n",
    "print(f\"Model created with {len(model.outputs)} outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style transfer functions\n",
    "def gram_matrix(input_tensor):\n",
    "    \"\"\"Calculate Gram matrix for style representation\"\"\"\n",
    "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "    input_shape = tf.shape(input_tensor)\n",
    "    num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)\n",
    "    return result / num_locations\n",
    "\n",
    "def get_style_content_loss(outputs, style_targets, content_targets, \n",
    "                          style_weight=1e-2, content_weight=1e4):\n",
    "    \"\"\"Calculate style and content loss\"\"\"\n",
    "    style_outputs = outputs[:num_style_layers]\n",
    "    content_outputs = outputs[num_style_layers:]\n",
    "    \n",
    "    # Style loss\n",
    "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[i] - style_targets[i])**2) \n",
    "                          for i in range(num_style_layers)])\n",
    "    style_loss *= style_weight / num_style_layers\n",
    "    \n",
    "    # Content loss\n",
    "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[i] - content_targets[i])**2) \n",
    "                            for i in range(num_content_layers)])\n",
    "    content_loss *= content_weight / num_content_layers\n",
    "    \n",
    "    total_loss = style_loss + content_loss\n",
    "    return total_loss, style_loss, content_loss\n",
    "\n",
    "def get_feature_representations(model, content_path, style_path):\n",
    "    \"\"\"Get style and content feature representations\"\"\"\n",
    "    # Load images\n",
    "    content_image = load_and_process_img(content_path)\n",
    "    style_image = load_and_process_img(style_path)\n",
    "    \n",
    "    # Preprocess for VGG\n",
    "    content_image = preprocess_input(content_image * 255)\n",
    "    style_image = preprocess_input(style_image * 255)\n",
    "    \n",
    "    # Get features\n",
    "    style_outputs = model(style_image)\n",
    "    content_outputs = model(content_image)\n",
    "    \n",
    "    # Get style features (Gram matrices)\n",
    "    style_features = [gram_matrix(style_output) for style_output in style_outputs[:num_style_layers]]\n",
    "    \n",
    "    # Get content features\n",
    "    content_features = [content_output for content_output in content_outputs[num_style_layers:]]\n",
    "    \n",
    "    return style_features, content_features\n",
    "\n",
    "print(\"‚úÖ Style transfer functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target features\n",
    "style_targets, content_targets = get_feature_representations(model, 'images/content.jpg', 'images/style.jpg')\n",
    "\n",
    "print(f\"Style targets: {len(style_targets)} layers\")\n",
    "print(f\"Content targets: {len(content_targets)} layers\")\n",
    "\n",
    "# Visualize Gram matrices\n",
    "fig, axes = plt.subplots(1, len(style_targets), figsize=(15, 3))\n",
    "for i, (ax, gram) in enumerate(zip(axes, style_targets)):\n",
    "    # Take a slice of the Gram matrix for visualization\n",
    "    gram_slice = gram[0, :min(64, gram.shape[1]), :min(64, gram.shape[2])]\n",
    "    im = ax.imshow(gram_slice, cmap='viridis')\n",
    "    ax.set_title(f'Gram Matrix\\nLayer {i+1}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('üé® Style Representation (Gram Matrices)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style transfer optimization\n",
    "@tf.function\n",
    "def train_step(image, model, style_targets, content_targets, \n",
    "               style_weight=1e-2, content_weight=1e4, total_variation_weight=30):\n",
    "    \"\"\"Single training step for style transfer\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(image)\n",
    "        total_loss, style_loss, content_loss = get_style_content_loss(\n",
    "            outputs, style_targets, content_targets, style_weight, content_weight)\n",
    "        \n",
    "        # Add total variation loss for smoothness\n",
    "        tv_loss = total_variation_weight * tf.image.total_variation(image)\n",
    "        total_loss += tv_loss\n",
    "    \n",
    "    grad = tape.gradient(total_loss, image)\n",
    "    return total_loss, style_loss, content_loss, tv_loss, grad\n",
    "\n",
    "def run_style_transfer(content_path, style_path, num_iterations=1000, \n",
    "                      style_weight=1e-2, content_weight=1e4):\n",
    "    \"\"\"Run style transfer optimization\"\"\"\n",
    "    # Initialize with content image\n",
    "    image = load_and_process_img(content_path)\n",
    "    image = tf.Variable(preprocess_input(image * 255), dtype=tf.float32)\n",
    "    \n",
    "    # Optimizer\n",
    "    opt = tf.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n",
    "    \n",
    "    # Store losses\n",
    "    losses = {'total': [], 'style': [], 'content': [], 'tv': []}\n",
    "    \n",
    "    print(f\"üé® Starting style transfer optimization...\")\n",
    "    print(f\"Iterations: {num_iterations}\")\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        total_loss, style_loss, content_loss, tv_loss, grads = train_step(\n",
    "            image, model, style_targets, content_targets, style_weight, content_weight)\n",
    "        \n",
    "        opt.apply_gradients([(grads, image)])\n",
    "        \n",
    "        # Clip pixel values\n",
    "        image.assign(tf.clip_by_value(image, -103.939, 255 - 123.68))\n",
    "        \n",
    "        # Store losses\n",
    "        losses['total'].append(total_loss.numpy())\n",
    "        losses['style'].append(style_loss.numpy())\n",
    "        losses['content'].append(content_loss.numpy())\n",
    "        losses['tv'].append(tv_loss.numpy())\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Total Loss = {total_loss:.2f}\")\n",
    "    \n",
    "    return image, losses\n",
    "\n",
    "# Run style transfer\n",
    "stylized_image, losses = run_style_transfer('images/content.jpg', 'images/style.jpg', \n",
    "                                           num_iterations=500)\n",
    "\n",
    "print(\"\\n‚úÖ Style transfer completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Original images and result\n",
    "axes[0, 0].imshow(deprocess_img(load_and_process_img('images/content.jpg').numpy()))\n",
    "axes[0, 0].set_title('üì∑ Content Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(deprocess_img(load_and_process_img('images/style.jpg').numpy()))\n",
    "axes[0, 1].set_title('üé® Style Image')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Stylized result\n",
    "result_img = deprocess_img(stylized_image.numpy())\n",
    "axes[1, 0].imshow(result_img)\n",
    "axes[1, 0].set_title('üñºÔ∏è Stylized Result')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Loss curves\n",
    "axes[1, 1].plot(losses['total'], label='Total Loss', alpha=0.8)\n",
    "axes[1, 1].plot(losses['style'], label='Style Loss', alpha=0.8)\n",
    "axes[1, 1].plot(losses['content'], label='Content Loss', alpha=0.8)\n",
    "axes[1, 1].set_title('üìâ Training Losses')\n",
    "axes[1, 1].set_xlabel('Iteration')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save result\n",
    "Image.fromarray(result_img).save('results/stylized_result.jpg')\n",
    "print(\"\\nüíæ Stylized image saved to 'results/stylized_result.jpg'\")\n",
    "\n",
    "print(f\"\\nüìä Final Losses:\")\n",
    "print(f\"Total Loss: {losses['total'][-1]:.2f}\")\n",
    "print(f\"Style Loss: {losses['style'][-1]:.2f}\")\n",
    "print(f\"Content Loss: {losses['content'][-1]:.2f}\")\n",
    "print(f\"TV Loss: {losses['tv'][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully implemented neural style transfer! Here's what you've accomplished:\n",
    "\n",
    "‚úÖ **Feature Extraction**: Used VGG19 to extract content and style features  \n",
    "‚úÖ **Gram Matrices**: Computed style representations  \n",
    "‚úÖ **Optimization**: Iteratively optimized image to match targets  \n",
    "‚úÖ **Artistic Transfer**: Created beautiful stylized images  \n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. Try different content and style images\n",
    "2. Experiment with different layer combinations\n",
    "3. Implement fast style transfer networks\n",
    "4. Move on to **Project 08: Variational Autoencoder (VAE)**\n",
    "\n",
    "Ready for generative modeling? Let's explore latent spaces! üåå"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
