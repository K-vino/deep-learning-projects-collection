{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒŒ Variational Autoencoder (VAE)\n",
    "\n",
    "Welcome to **Variational Autoencoders**! In this notebook, we'll explore probabilistic generative modeling, learn meaningful latent representations, and generate new data from learned distributions.\n",
    "\n",
    "## What you'll learn:\n",
    "- Probabilistic encoder-decoder architecture\n",
    "- Reparameterization trick for backpropagation\n",
    "- KL divergence and ELBO optimization\n",
    "- Latent space exploration and interpolation\n",
    "\n",
    "Let's explore the latent space! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape to flatten\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"Data range: [{x_train.min():.2f}, {x_train.max():.2f}]\")\n",
    "\n",
    "# Visualize sample digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = x_train[i].reshape(28, 28)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'Digit: {y_train[i]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('ðŸ”¢ MNIST Dataset Samples', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Architecture\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, latent_dim=20, intermediate_dim=512, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(784,)),\n",
    "            layers.Dense(intermediate_dim, activation='relu'),\n",
    "            layers.Dense(intermediate_dim, activation='relu'),\n",
    "        ])\n",
    "        \n",
    "        # Latent space parameters\n",
    "        self.z_mean = layers.Dense(latent_dim)\n",
    "        self.z_log_var = layers.Dense(latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            layers.Dense(intermediate_dim, activation='relu'),\n",
    "            layers.Dense(intermediate_dim, activation='relu'),\n",
    "            layers.Dense(784, activation='sigmoid'),\n",
    "        ])\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent parameters\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        z_mean = self.z_mean(h)\n",
    "        z_log_var = self.z_log_var(h)\n",
    "        return z_mean, z_log_var\n",
    "    \n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        \"\"\"Reparameterization trick\"\"\"\n",
    "        batch_size = tf.shape(z_mean)[0]\n",
    "        epsilon = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent code to reconstruction\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        z_mean, z_log_var = self.encode(x)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        reconstruction = self.decode(z)\n",
    "        return reconstruction, z_mean, z_log_var\n",
    "\n",
    "# Create VAE model\n",
    "LATENT_DIM = 20\n",
    "vae = VAE(latent_dim=LATENT_DIM)\n",
    "\n",
    "print(f\"âœ… VAE model created with latent dimension: {LATENT_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Loss Function\n",
    "def vae_loss(x, reconstruction, z_mean, z_log_var):\n",
    "    \"\"\"VAE loss = Reconstruction loss + KL divergence\"\"\"\n",
    "    # Reconstruction loss (binary crossentropy)\n",
    "    reconstruction_loss = binary_crossentropy(x, reconstruction)\n",
    "    reconstruction_loss = tf.reduce_sum(reconstruction_loss, axis=1)\n",
    "    \n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * tf.reduce_sum(\n",
    "        1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1\n",
    "    )\n",
    "    \n",
    "    return tf.reduce_mean(reconstruction_loss + kl_loss), tf.reduce_mean(reconstruction_loss), tf.reduce_mean(kl_loss)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Training step\n",
    "@tf.function\n",
    "def train_step(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        reconstruction, z_mean, z_log_var = vae(x)\n",
    "        total_loss, recon_loss, kl_loss = vae_loss(x, reconstruction, z_mean, z_log_var)\n",
    "    \n",
    "    gradients = tape.gradient(total_loss, vae.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, vae.trainable_variables))\n",
    "    \n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "print(\"âœ… Loss function and training step defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "\n",
    "# Track losses\n",
    "train_losses = {'total': [], 'reconstruction': [], 'kl': []}\n",
    "\n",
    "print(f\"ðŸš€ Starting VAE training...\")\n",
    "print(f\"Epochs: {EPOCHS}, Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_losses = {'total': [], 'reconstruction': [], 'kl': []}\n",
    "    \n",
    "    for batch in train_dataset:\n",
    "        total_loss, recon_loss, kl_loss = train_step(batch)\n",
    "        epoch_losses['total'].append(total_loss)\n",
    "        epoch_losses['reconstruction'].append(recon_loss)\n",
    "        epoch_losses['kl'].append(kl_loss)\n",
    "    \n",
    "    # Average losses for epoch\n",
    "    avg_total = tf.reduce_mean(epoch_losses['total'])\n",
    "    avg_recon = tf.reduce_mean(epoch_losses['reconstruction'])\n",
    "    avg_kl = tf.reduce_mean(epoch_losses['kl'])\n",
    "    \n",
    "    train_losses['total'].append(avg_total)\n",
    "    train_losses['reconstruction'].append(avg_recon)\n",
    "    train_losses['kl'].append(avg_kl)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Total Loss = {avg_total:.2f}, \"\n",
    "              f\"Recon Loss = {avg_recon:.2f}, KL Loss = {avg_kl:.2f}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(train_losses['total'], label='Total Loss', alpha=0.8)\n",
    "axes[0].plot(train_losses['reconstruction'], label='Reconstruction Loss', alpha=0.8)\n",
    "axes[0].plot(train_losses['kl'], label='KL Divergence', alpha=0.8)\n",
    "axes[0].set_title('ðŸ“‰ VAE Training Losses')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test reconstructions\n",
    "test_sample = x_test[:10]\n",
    "reconstructions, _, _ = vae(test_sample)\n",
    "\n",
    "# Show original vs reconstructed\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('ðŸ”„ Original vs Reconstructed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed reconstruction comparison\n",
    "fig, axes = plt.subplots(2, 10, figsize=(20, 6))\n",
    "\n",
    "for i in range(10):\n",
    "    # Original\n",
    "    axes[0, i].imshow(test_sample[i].numpy().reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].set_title('Original')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Reconstructed\n",
    "    axes[1, i].imshow(reconstructions[i].numpy().reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].set_title('Reconstructed')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('ðŸ”„ VAE Reconstructions', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Training Results:\")\n",
    "print(f\"Total Loss: {train_losses['total'][-1]:.2f}\")\n",
    "print(f\"Reconstruction Loss: {train_losses['reconstruction'][-1]:.2f}\")\n",
    "print(f\"KL Divergence: {train_losses['kl'][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new samples\n",
    "def generate_samples(vae, num_samples=16):\n",
    "    \"\"\"Generate new samples from random latent codes\"\"\"\n",
    "    random_latent = tf.random.normal(shape=(num_samples, vae.latent_dim))\n",
    "    generated = vae.decode(random_latent)\n",
    "    return generated\n",
    "\n",
    "# Generate and visualize samples\n",
    "generated_samples = generate_samples(vae, 16)\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = generated_samples[i].numpy().reshape(28, 28)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('ðŸŽ² Generated Samples from Random Latent Codes', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent space visualization (2D projection)\n",
    "if LATENT_DIM > 2:\n",
    "    # Encode test samples\n",
    "    test_sample = x_test[:1000]\n",
    "    test_labels = y_test[:1000]\n",
    "    z_mean, _ = vae.encode(test_sample)\n",
    "    \n",
    "    # Use t-SNE for 2D projection\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    z_2d = tsne.fit_transform(z_mean.numpy())\n",
    "    \n",
    "    # Plot latent space\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(z_2d[:, 0], z_2d[:, 1], c=test_labels, cmap='tab10', alpha=0.7)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('ðŸŒŒ Latent Space Visualization (t-SNE projection)')\n",
    "    plt.xlabel('Latent Dimension 1')\n",
    "    plt.ylabel('Latent Dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    # Direct 2D visualization\n",
    "    test_sample = x_test[:1000]\n",
    "    test_labels = y_test[:1000]\n",
    "    z_mean, _ = vae.encode(test_sample)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(z_mean[:, 0], z_mean[:, 1], c=test_labels, cmap='tab10', alpha=0.7)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('ðŸŒŒ 2D Latent Space')\n",
    "    plt.xlabel('Latent Dimension 1')\n",
    "    plt.ylabel('Latent Dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent space interpolation\n",
    "def interpolate_latent(vae, start_img, end_img, steps=10):\n",
    "    \"\"\"Interpolate between two images in latent space\"\"\"\n",
    "    # Encode images to latent space\n",
    "    start_z, _ = vae.encode(start_img.reshape(1, -1))\n",
    "    end_z, _ = vae.encode(end_img.reshape(1, -1))\n",
    "    \n",
    "    # Interpolate\n",
    "    interpolated_images = []\n",
    "    for i in range(steps):\n",
    "        alpha = i / (steps - 1)\n",
    "        interpolated_z = (1 - alpha) * start_z + alpha * end_z\n",
    "        decoded = vae.decode(interpolated_z)\n",
    "        interpolated_images.append(decoded[0].numpy())\n",
    "    \n",
    "    return interpolated_images\n",
    "\n",
    "# Select two different digits for interpolation\n",
    "start_idx = np.where(y_test == 0)[0][0]\n",
    "end_idx = np.where(y_test == 9)[0][0]\n",
    "\n",
    "start_img = x_test[start_idx]\n",
    "end_img = x_test[end_idx]\n",
    "\n",
    "interpolated = interpolate_latent(vae, start_img, end_img, 10)\n",
    "\n",
    "# Visualize interpolation\n",
    "fig, axes = plt.subplots(1, 10, figsize=(20, 4))\n",
    "for i, (ax, img) in enumerate(zip(axes, interpolated)):\n",
    "    ax.imshow(img.reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f'Step {i+1}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('ðŸŒˆ Latent Space Interpolation (0 â†’ 9)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š VAE Summary:\")\n",
    "print(f\"Latent Dimension: {LATENT_DIM}\")\n",
    "print(f\"Training Epochs: {EPOCHS}\")\n",
    "print(f\"Final Loss: {train_losses['total'][-1]:.2f}\")\n",
    "print(f\"Model Parameters: {vae.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've successfully implemented and trained a Variational Autoencoder! Here's what you've accomplished:\n",
    "\n",
    "âœ… **VAE Architecture**: Built probabilistic encoder-decoder  \n",
    "âœ… **Reparameterization Trick**: Enabled backpropagation through sampling  \n",
    "âœ… **ELBO Optimization**: Balanced reconstruction and regularization  \n",
    "âœ… **Latent Space**: Explored meaningful representations  \n",
    "âœ… **Generation**: Created new samples from learned distribution  \n",
    "âœ… **Interpolation**: Smooth transitions in latent space  \n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "1. Try Î²-VAE for better disentanglement\n",
    "2. Implement Conditional VAE for controlled generation\n",
    "3. Experiment with different latent dimensions\n",
    "4. Move on to **Project 09: Transformer for Language Modeling**\n",
    "\n",
    "Ready for the attention revolution? Let's build Transformers! ðŸ¤–"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
