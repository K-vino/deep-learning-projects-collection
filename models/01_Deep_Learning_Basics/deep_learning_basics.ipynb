{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Deep Learning Basics\n",
    "\n",
    "Welcome to your first deep learning project! In this notebook, we'll explore the fundamental concepts of neural networks through three hands-on implementations:\n",
    "\n",
    "1. **Linear Regression** - Understanding the basics\n",
    "2. **Perceptron** - The simplest neural network\n",
    "3. **MNIST Classification** - Multi-class classification\n",
    "\n",
    "Let's start our journey into the world of deep learning! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"ðŸ“š Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Part 1: Linear Regression from Scratch\n",
    "\n",
    "Let's start with the simplest form of a neural network - linear regression. We'll implement it from scratch to understand the core concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic regression data\n",
    "X_reg, y_reg = make_regression(n_samples=1000, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train_reg, y_train_reg, alpha=0.6, label='Training Data')\n",
    "plt.scatter(X_test_reg, y_test_reg, alpha=0.6, label='Test Data')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.title('ðŸ“Š Regression Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training samples: {len(X_train_reg)}\")\n",
    "print(f\"Test samples: {len(X_test_reg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.costs = []\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000):\n",
    "        # Initialize parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.random.normal(0, 0.01, n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # Compute cost (MSE)\n",
    "            cost = np.mean((y_pred - y) ** 2)\n",
    "            self.costs.append(cost)\n",
    "            \n",
    "            # Backward pass (compute gradients)\n",
    "            dw = (2/n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (2/n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Cost: {cost:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "# Train our linear regression model\n",
    "model_reg = LinearRegression(learning_rate=0.01)\n",
    "model_reg.fit(X_train_reg, y_train_reg, epochs=1000)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_reg = model_reg.predict(X_test_reg)\n",
    "\n",
    "# Calculate RÂ² score\n",
    "r2_score = 1 - (np.sum((y_test_reg - y_pred_reg) ** 2) / np.sum((y_test_reg - np.mean(y_test_reg)) ** 2))\n",
    "print(f\"\\nðŸŽ¯ RÂ² Score: {r2_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Training progress\n",
    "ax1.plot(model_reg.costs)\n",
    "ax1.set_title('ðŸ“ˆ Training Progress')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Cost (MSE)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Predictions vs actual\n",
    "ax2.scatter(X_test_reg, y_test_reg, alpha=0.6, label='Actual')\n",
    "ax2.scatter(X_test_reg, y_pred_reg, alpha=0.6, label='Predicted')\n",
    "ax2.plot(X_test_reg, y_pred_reg, 'r-', alpha=0.8, label='Regression Line')\n",
    "ax2.set_title(f'ðŸŽ¯ Predictions (RÂ² = {r2_score:.3f})')\n",
    "ax2.set_xlabel('Feature')\n",
    "ax2.set_ylabel('Target')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Part 2: Perceptron Implementation\n",
    "\n",
    "Now let's implement a perceptron - the building block of neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate binary classification data\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=1000, n_features=2, n_redundant=0, n_informative=2,\n",
    "    n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_class = scaler.fit_transform(X_train_class)\n",
    "X_test_class = scaler.transform(X_test_class)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red', 'blue']\n",
    "for i in range(2):\n",
    "    plt.scatter(X_train_class[y_train_class == i, 0], \n",
    "               X_train_class[y_train_class == i, 1], \n",
    "               c=colors[i], alpha=0.6, label=f'Class {i}')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('ðŸ“Š Binary Classification Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training samples: {len(X_train_class)}\")\n",
    "print(f\"Test samples: {len(X_test_class)}\")\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"class Perceptron:\\n\",\n",
    "    \"    def __init__(self, learning_rate=0.01):\\n\",\n",
    "    \"        self.learning_rate = learning_rate\\n\",\n",
    "    \"        self.weights = None\\n\",\n",
    "    \"        self.bias = None\\n\",\n",
    "    \"        self.errors = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def sigmoid(self, x):\\n\",\n",
    "    \"        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def fit(self, X, y, epochs=1000):\\n\",\n",
    "    \"        n_samples, n_features = X.shape\\n\",\n",
    "    \"        self.weights = np.random.normal(0, 0.01, n_features)\\n\",\n",
    "    \"        self.bias = 0\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for epoch in range(epochs):\\n\",\n",
    "    \"            # Forward pass\\n\",\n",
    "    \"            linear_pred = np.dot(X, self.weights) + self.bias\\n\",\n",
    "    \"            y_pred = self.sigmoid(linear_pred)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Compute error\\n\",\n",
    "    \"            error = np.mean((y_pred - y) ** 2)\\n\",\n",
    "    \"            self.errors.append(error)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Backward pass\\n\",\n",
    "    \"            dw = (1/n_samples) * np.dot(X.T, (y_pred - y) * y_pred * (1 - y_pred))\\n\",\n",
    "    \"            db = (1/n_samples) * np.sum((y_pred - y) * y_pred * (1 - y_pred))\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Update parameters\\n\",\n",
    "    \"            self.weights -= self.learning_rate * dw\\n\",\n",
    "    \"            self.bias -= self.learning_rate * db\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            if epoch % 100 == 0:\\n\",\n",
    "    \"                print(f\\\"Epoch {epoch}, Error: {error:.4f}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def predict(self, X):\\n\",\n",
    "    \"        linear_pred = np.dot(X, self.weights) + self.bias\\n\",\n",
    "    \"        return (self.sigmoid(linear_pred) > 0.5).astype(int)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def predict_proba(self, X):\\n\",\n",
    "    \"        linear_pred = np.dot(X, self.weights) + self.bias\\n\",\n",
    "    \"        return self.sigmoid(linear_pred)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Train perceptron\\n\",\n",
    "    \"perceptron = Perceptron(learning_rate=0.1)\\n\",\n",
    "    \"perceptron.fit(X_train_class, y_train_class, epochs=1000)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Make predictions\\n\",\n",
    "    \"y_pred_class = perceptron.predict(X_test_class)\\n\",\n",
    "    \"accuracy = accuracy_score(y_test_class, y_pred_class)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nðŸŽ¯ Perceptron Accuracy: {accuracy:.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## ðŸ§  Part 3: MNIST Neural Network with TensorFlow\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now let's build a proper neural network for the famous MNIST dataset!\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load MNIST dataset\\n\",\n",
    "    \"(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = keras.datasets.mnist.load_data()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Normalize pixel values to [0, 1]\\n\",\n",
    "    \"X_train_mnist = X_train_mnist.astype('float32') / 255.0\\n\",\n",
    "    \"X_test_mnist = X_test_mnist.astype('float32') / 255.0\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Reshape data (flatten images)\\n\",\n",
    "    \"X_train_mnist = X_train_mnist.reshape(X_train_mnist.shape[0], -1)\\n\",\n",
    "    \"X_test_mnist = X_test_mnist.reshape(X_test_mnist.shape[0], -1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Convert labels to categorical\\n\",\n",
    "    \"y_train_mnist = keras.utils.to_categorical(y_train_mnist, 10)\\n\",\n",
    "    \"y_test_mnist = keras.utils.to_categorical(y_test_mnist, 10)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Training data shape: {X_train_mnist.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Training labels shape: {y_train_mnist.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Test data shape: {X_test_mnist.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Test labels shape: {y_test_mnist.shape}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualize some MNIST samples\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 5, figsize=(12, 6))\\n\",\n",
    "    \"for i, ax in enumerate(axes.flat):\\n\",\n",
    "    \"    # Reshape back to 28x28 for visualization\\n\",\n",
    "    \"    image = X_train_mnist[i].reshape(28, 28)\\n\",\n",
    "    \"    ax.imshow(image, cmap='gray')\\n\",\n",
    "    \"    ax.set_title(f'Label: {np.argmax(y_train_mnist[i])}')\\n\",\n",
    "    \"    ax.axis('off')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.suptitle('ðŸ”¢ MNIST Dataset Samples', fontsize=16)\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Build neural network model\\n\",\n",
    "    \"model = keras.Sequential([\\n\",\n",
    "    \"    layers.Dense(128, activation='relu', input_shape=(784,)),\\n\",\n",
    "    \"    layers.Dropout(0.2),\\n\",\n",
    "    \"    layers.Dense(64, activation='relu'),\\n\",\n",
    "    \"    layers.Dropout(0.2),\\n\",\n",
    "    \"    layers.Dense(10, activation='softmax')\\n\",\n",
    "    \"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Compile model\\n\",\n",
    "    \"model.compile(\\n\",\n",
    "    \"    optimizer='adam',\\n\",\n",
    "    \"    loss='categorical_crossentropy',\\n\",\n",
    "    \"    metrics=['accuracy']\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display model architecture\\n\",\n",
    "    \"model.summary()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Train the model\\n\",\n",
    "    \"history = model.fit(\\n\",\n",
    "    \"    X_train_mnist, y_train_mnist,\\n\",\n",
    "    \"    batch_size=128,\\n\",\n",
    "    \"    epochs=10,\\n\",\n",
    "    \"    validation_split=0.1,\\n\",\n",
    "    \"    verbose=1\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Evaluate on test set\\n\",\n",
    "    \"test_loss, test_accuracy = model.evaluate(X_test_mnist, y_test_mnist, verbose=0)\\n\",\n",
    "    \"print(f\\\"\\\\nðŸŽ¯ Test Accuracy: {test_accuracy:.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot training history\\n\",\n",
    "    \"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot training & validation accuracy\\n\",\n",
    "    \"ax1.plot(history.history['accuracy'], label='Training Accuracy')\\n\",\n",
    "    \"ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\\n\",\n",
    "    \"ax1.set_title('ðŸ“ˆ Model Accuracy')\\n\",\n",
    "    \"ax1.set_xlabel('Epoch')\\n\",\n",
    "    \"ax1.set_ylabel('Accuracy')\\n\",\n",
    "    \"ax1.legend()\\n\",\n",
    "    \"ax1.grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot training & validation loss\\n\",\n",
    "    \"ax2.plot(history.history['loss'], label='Training Loss')\\n\",\n",
    "    \"ax2.plot(history.history['val_loss'], label='Validation Loss')\\n\",\n",
    "    \"ax2.set_title('ðŸ“‰ Model Loss')\\n\",\n",
    "    \"ax2.set_xlabel('Epoch')\\n\",\n",
    "    \"ax2.set_ylabel('Loss')\\n\",\n",
    "    \"ax2.legend()\\n\",\n",
    "    \"ax2.grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## ðŸŽ‰ Congratulations!\\n\",\n",
    "    \"\\n\",\n",
    "    \"You've successfully completed your first deep learning project! Here's what you've learned:\\n\",\n",
    "    \"\\n\",\n",
    "    \"âœ… **Linear Regression**: Understanding the basics of neural networks  \\n\",\n",
    "    \"âœ… **Perceptron**: Building the simplest neural network from scratch  \\n\",\n",
    "    \"âœ… **MNIST Classification**: Multi-class classification with TensorFlow  \\n\",\n",
    "    \"\\n\",\n",
    "    \"### ðŸš€ Next Steps:\\n\",\n",
    "    \"1. Experiment with different architectures\\n\",\n",
    "    \"2. Try different activation functions\\n\",\n",
    "    \"3. Implement regularization techniques\\n\",\n",
    "    \"4. Move on to **Project 02: CIFAR-10 CNN Classifier**\\n\",\n",
    "    \"\\n\",\n",
    "    \"Keep learning and happy coding! ðŸŽ¯\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.10.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
