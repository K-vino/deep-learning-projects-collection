# Project 01: Deep Learning Basics

## ğŸ¯ Project Overview

This project introduces the fundamental concepts of deep learning through three key implementations:
1. **Linear Regression** - Understanding the basics of neural networks
2. **Perceptron** - Building the simplest neural network
3. **MNIST Classification** - Multi-class classification with a simple neural network

## ğŸ“š Learning Objectives

By completing this project, you will:
- Understand the basic building blocks of neural networks
- Learn how to implement forward and backward propagation
- Work with the famous MNIST dataset
- Visualize training progress and model performance
- Compare different activation functions and optimizers

## ğŸ› ï¸ Technologies Used

- **Python 3.10+**
- **TensorFlow/Keras** - Deep learning framework
- **NumPy** - Numerical computations
- **Matplotlib** - Data visualization
- **Scikit-learn** - Data preprocessing and metrics

## ğŸ“ Project Structure

```
01_Deep_Learning_Basics/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ deep_learning_basics.ipynb   # Main notebook
â””â”€â”€ assets/                      # Generated plots and visualizations
```

## ğŸš€ Getting Started

### Prerequisites

```bash
pip install tensorflow numpy matplotlib scikit-learn jupyter
```

### Running the Project

1. Open `deep_learning_basics.ipynb` in Jupyter Notebook, Google Colab, or Kaggle
2. Run all cells sequentially
3. Experiment with different hyperparameters
4. Observe how changes affect model performance

## ğŸ“Š What You'll Build

### 1. Linear Regression
- Implement a simple linear regression from scratch
- Visualize the learning process
- Compare with scikit-learn implementation

### 2. Perceptron
- Build a single-layer perceptron
- Understand the limitations of linear classifiers
- Visualize decision boundaries

### 3. MNIST Neural Network
- Create a multi-layer neural network
- Train on the MNIST handwritten digits dataset
- Achieve >95% accuracy on test data
- Visualize misclassified examples

## ğŸ¯ Expected Results

- **Linear Regression**: RÂ² score > 0.85
- **Perceptron**: Accuracy > 90% on linearly separable data
- **MNIST Classification**: Test accuracy > 95%

## ğŸ” Key Concepts Covered

- **Forward Propagation**: How data flows through the network
- **Backpropagation**: How gradients are computed and weights updated
- **Loss Functions**: Mean Squared Error, Cross-entropy
- **Activation Functions**: Sigmoid, ReLU, Softmax
- **Optimizers**: SGD, Adam
- **Regularization**: Dropout, L2 regularization

## ğŸ“ˆ Visualizations

The notebook includes:
- Training/validation loss curves
- Accuracy progression over epochs
- Confusion matrices
- Sample predictions with confidence scores
- Weight visualizations

## ğŸ“ Next Steps

After completing this project:
1. Experiment with different network architectures
2. Try different activation functions
3. Implement early stopping
4. Move on to Project 02: CIFAR-10 CNN Classifier

## ğŸ“š Additional Resources

- [Deep Learning Specialization - Coursera](https://www.coursera.org/specializations/deep-learning)
- [Neural Networks and Deep Learning - Michael Nielsen](http://neuralnetworksanddeeplearning.com/)
- [TensorFlow Tutorials](https://www.tensorflow.org/tutorials)

## ğŸ¤ Contributing

Found an issue or want to improve this project? Feel free to submit a pull request!

---

**Happy Learning! ğŸš€**
